<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Blog</title>
    <link rel="stylesheet" href="{{ url_for('static', filename = 'css/style.css') }}">
  </head>

  <body>
    <h1>Détecter des Bad Buzz Grâce au Deep Learning</h1>

    <h3>Introduction</h3>
    <p>Voici un article sur la manière dont on va tenter de détecter si des tweets sont associé à un sentiment négatif ou positif. C'est un exemple assez classique d'apprentissage sur du texte, mais qui peut avoir de nombreuses utilisation, par exemple pour détecter des bad buzz.</p>
    <p>Pour réaliser cette analyse, on va s'assister d'une base de données de 1.600.000 tweets déjà évalués comme positifs ou négatifs afin de réaliser de l'apprentissage supervisé pour notre modèle. Cette base de données étant conséquente nous n'en prendrons qu'une partie pour réaliser nos comparaisons dans un premier temps (5%), puis nous lancerons l'entraînement sur tous les tweets une fois que nous aurons décidé de notre modèle. Notons également que les tweets sont également répartis entre ceux évalués comme positifs et ceux évalués comme négatifs, ce qui nous simplifiera l'apprentissage et les comparaisons.</p>

    <h3>Traitement du texte</h3>
    <p>Pour commencer, il va falloir "nettoyer le texte". En effet, afin d'améliorer les performances, on doit d'abord se débarrasser d'informations peu ou pas utiles pour notre modèle. Par exemple, les termes de notre phrase commençant par <code>'@'</code> concernent le tag d'une personne. Ils ne sont pas utile pour déterminer si un tweet est positif ou négatif. En revanche, on pourrait s'en servir pour cibler les tweets à sélectionner dans le cas d'une détection de bad buzz. Mais cela ne concerne pas notre partie du projet.</p>
    <p>Comme autre traitements assez classiques, on va se débarasser de la ponctuation, des liens... Et enfin des mots trop rarement utilisés et des mots ne comportant qu'une seule lettre.</p>
    <p>Une dernière étape de traitement consiste à lemmatiser le texte, c'est-à-dire transformer des mots dans une forme plus générale afin de faciliter la reconnaissance par notre modèle. L'idée est que des verbes seront traités de la même manière peu importe leurs conjugaisons, ou bien encore que des mots comme <code>'happier'</code> et <code>'happy'</code> soit reconnus de la même manière par le modèle.</p>
    <p>Par exemple la phrase <code>'@someone_pseudo I can't believe it, I am happier than ever!'</code> sera finalement traitée comme <code>'can not believe it be happy than ever'</code></p>

    <h3>Modèle simple</h3>
    <p>Avant de se lancer sur différents modèles de deep learning, nous allons réaliser l'apprentissage à l'aide d'un modèle plus classique se basant sur un tf-idf (méthode de pondération permettant d'exploiter des textes sous forme d'un vecteur de nombres utilisable pour un modèle de machine learning, <a href="https://fr.wikipedia.org/wiki/TF-IDF" target="_blank">Wikipédia</a>). Puis exploitant une classification à vecteur de support, SVC, un modèle linéaire relativement efficace pour effectuer de la classification sur du texte. <a href="https://fr.wikipedia.org/wiki/Machine_%C3%A0_vecteurs_de_support" target="_blank">Wikipédia</a></p>
    <p>De cette manière on obtient un modèle assez long à entraîner et consommant une place importante en mémoire. Mais cela nous permet d'avoir une référence en terme de résulat.</p>
    <p>Pour analyser les performances de nos différents modèles, nous allons simplement utiliser la métrique de précision. En effet, la base de données étant parfaitement équilibrée et étant donné que nous n'avons pas ici d'intérêt à privilégier les résultats sur les tweets positifs ou négatifs, il n'est pas nécessaire de regarder d'autres métriques telles que l'AUC ou le f-beta score.</p>
    <p>Et donc pour ce premier modèle, nous obtenons une précision de 76,10%.</p>

    <h3>RNN, Comparaison de différents nettoyages</h3>
    <p>Nous allons désormais nous baser sur des réseaux de neurones récurrents (RNN). Ces derniers plus récents ont tendances à fournir de meilleurs résultats pour les tâches de classification de texte. Et pour commencer nos comparaisons, nous allons utiliser un modèle avec une couche LSTM unilatérale suivie d'une couche Dense. (plus d'infos sur les couche LSTM <a href="https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47">ici</a>)</p>
    <p>On peut commencer par vérifier si notre manière de nettoyer le texte ne pourrait pas être améliorée. Pour cela nous allons comparer notre traitements à deux variantes, en utilisant un stemmer à la place d'un lemmatizer, et en supprimant les stop-words.</p>

    <h5>Résultats</h5>
    <table>
        <tr>
            <td><b>Méthode</b></td>
            <td><b>Précision</b></td>
        </tr>
        <tr>
            <td>Lemmatizer avec stopwords</td>
            <td>78,34%</td>
        </tr>
        <tr>
            <td>Stemmer avec stopwords</td>
            <td>76,40%</td>
        </tr>
        <tr>
            <td>Lemmatizer sans stopwords</td>
            <td>76,11%</td>
        </tr>
    </table>

    <p>Comme on peut le voir, parmi les différents tests, notre première approche de nettoyage était la meilleure. On va donc la conserver pour la suite.</p>

    <h3>RNN, Comparaison de différentes architectures</h3>
    <p>Pour notre modèle, on peut ensuite analyser différentes architectures possibles. Pour commencer, on peut regarder ce que donnerait un réseau de neurones sans couche LSTM, puis en testant différentes combinaisons de couches LSTM</p>

    <h5>Résultats</h5>
    <table>
        <tr>
            <td><b>Architecture</b></td>
            <td><b>Précision</b></td>
        </tr>
        <tr>
            <td>Sans couche LSTM</td>
            <td>77,27%</td>
        </tr>
        <tr>
            <td>Une couche LSTM unilatérale</td>
            <td>78,34%</td>
        </tr>
        <tr>
            <td>Une couche LSTM bilatérale</td>
            <td>78,48%</td>
        </tr>
        <tr>
            <td>Deux couches LSTM unilatérales</td>
            <td>78,11%</td>
        </tr>
        <tr>
            <td>Deux couches LSTM bilatérales</td>
            <td>78,13%</td>
        </tr>
    </table>

    <p>On observe qu'ajouter une couche LSTM n'est pas très intéressant ici, en plus d'alourdir le modèle, cela diminue légèrement les performances. On pourrait observer un phénomène similaire en ajoutant une couche Dense. Ensuite, la caractéristique unilatérale ou bilatérale ne change pas vraiment les résultats de manière significative, cela peut s'expliquer par le fait que les tweets sont des textes courts, limitant l'intérêt du caractère bilatéral.</p>
    <p>Précisons également pour ajouter l'intérêt de la couche LSTM, La vitesse de convergence du modèle sans cette couche est sensiblement ralentie.</p>

    <h3>Word2Vec</h3>
    <p>Une autre manière d'améliorer ce modèle serait d'utiliser une méthode de plongement de mots comme première couche. Cela permettrait de <em>rapprocher</em> les mots similaires pour notre analyse afin que l'algorithme apprenne plus facilement. Par exemple <code>"Le roi mange"</code> et <code>"Le prince mange"</code> seraient reconnues comme étant proches simplifiant la classification pour notre modèle.</p>
    <p>Pour commencer on pourrait utiliser un modèle de type Word2vec. Mais malheureusement, un tel modèle entraîné sur des tweets ne permet pas de renvoyer des résultats forcément pertinents. Le mot le plus proche de <code>'man</code> est ici <code>'awww'</code>... La précision résultant de ce modèle est de 76,54%</p>

    <h3>Glove Embedding</h3>
    <p>Une manière de pallier ce problème serait d'utiliser un modèle pré-entraîné. Ici nous utiliserons le modèle Glove.</p>
    <p>Ainsi, on arrive à augmenter la précision des résultats à 78,80%. Ce qui est d'autant plus prometteur que ces modèles a également de meilleurs espoirs d'améliorations en utilisant plus de données d'entraînement.</p>

    <h3>Modèle BERT</h3>
    <p>Enfin, pour terminer notre analyse, il convient de s'intéresser au modèle le plus performant à ce jour. Le modèle BERT. Il s'agit d'un modèle developpé par Google en 2018, et est très performants sur des tâches variées concernant le traitement de textes.</p>
    <p>Grâce à ce modèle, nous arrivons à atteindre des performances de 82,81%, ce qui est notablemment supérieur aux modèles précédents.</p>

    <h3>Analyse de nos résultats</h3>
    <p>Il convient de regarder quelques résultats pour analyser nos comparaisons. Voici quelques tweets mal prédis par notre modèle :</p>
    <table>
        <tr>
            <td class="tweets"><code>lacks inspiration</code></td>
            <td class="tweets"><em>faussement prédit positif</em></td>
        </tr>
        <tr>
            <td class="tweets"><code>@Exxx tsk - I just hoose the Amazon reviewer who is the most sycophantic - <br>not enough gush, and its clearly not worth buying</code></td>
            <td class="tweets"><em>faussement prédit négatif</em></td>
        </tr>
        <tr>
            <td class="tweets"><code>@funkypancake Ah, how I miss the English pub!</code></td>
            <td class="tweets"><em>faussement prédit négatif</em></td>
        </tr>
    </table>

    <p>Ici on s'aperçoit de plusieurs cas de mauvaises prédictions comparées au résultat donné par le dataset initial</p>

    <p>Le premier tweet est une simple erreur de notre modèle, il montre que du progrès est encore réalisable.</p>
    <p>La mauvaise prédiction du deuxième par contre est due à un mauvais étiquettage. On pourrait s'attendre à ce que la plupart des humains considèrent ce tweet comme négatif. Par conséquent, l'amélioration ici n'est pas à faire du côté du modèle mais du dataset qui n'est pas parfait.</p>
    <p>Et enfin, une part des tweets est ambigue comme pour le troisième par exemple, on ne peut pas vraiment repprocher à notre modèle de s'être trompé ici.</p>
    <p>Ces deux dernières catégories montrent qu'il est potentiellement difficile d'améliorer notre algorithme sans retravailler notre jeu de données dans un premier temps.</p>


  </body>
</html>